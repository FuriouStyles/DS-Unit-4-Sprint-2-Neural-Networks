{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stephen P LS_DS_431_Intro_to_NN_Assignment.ipynb","provenance":[{"file_id":"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb","timestamp":1586396200985}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dVfaLrjLvxvQ"},"source":["<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n","<br></br>\n","<br></br>\n","\n","# Neural Networks\n","\n","## *Data Science Unit 4 Sprint 2 Assignment 1*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wxtoY12mwmih"},"source":["## Define the Following:\n","You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n","\n","### Input Layer:\n","Input layers contain the data thats in our dataset. It's akin to the X features that are used in the sklearn pipeline. To avoid working with numbers that are extremely big inside the network, it's important to scale the data before training to cut down on compute time.\n","### Hidden Layer:\n","Hidden layers are nodes inside the neural network that can only be accessed by the input layers. The value of the hidden layers are the the sum of the input layers and the weights associated between the layers. You don't always have to have the same number of nodes in each hidden layer; there can be a growing or shrinking of the number of hidden layers as a function of the layers and weights before them. Hidden layers are what allow NNs to achieve their accuracy, where after many epochs of training the NN is able to finely tune the weights and biases to achieve the lowest possible error relative to the target.\n","### Output Layer:\n","Output layers are the target of a NN, akin to the Y target feature in an sklearn pipeline. They are the value(s) that you are trying to predict with the network. For a regression problem, you will typically have a single node in the output layer, but for classification problems you will have a number of nodes representing each class.\n","### Neuron:\n","A Neuron is a single biological neuron on which the perceptron and other NN nodes are modeled. It takes an input and passes that input on to the next based on certain factors. \n","### Weight:\n","The weight represents the strength of the connection between nodes. During the first epoch the weights and biases are set at random values, and the error between our expected training set output and the actual output of the network affects how the weights and biases are tuned for the next epoch. This is done during backpropogation.\n","### Activation Function:\n","The activation function is a function that reduces the summed total of weights and inputs into a node to a number between 1 and 10. Doing this reduces computational demand by the network while ensuring the importances of the weights between the nodes is not lost, and also produces non-linear results. You can set different activation functions for different layers and even different nodes in a layer, but the use cases of doing so are limited in number.\n","### Node Map:\n","Node maps are handy visual representations of the way the nodes in each layer connect with one another. I suppose you could also include the initializtion weights and biases in a Node map as well.\n","### Perceptron:\n","Perceptrons are the simplest and first forms of NNs that takes inputs, modifies them by a weight, reduces them (or not) by an activation function, and then spits out an output.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NXuy9WcWzxa4"},"source":["## Inputs -> Outputs\n","\n","### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PlSwIJMC0A8F"},"source":["Each feature in the data set is an input node in the input layer (x). The values of these input nodes are then multiplied by the weights and summed (xw). To avoid liniarity, this weighted sum is them passed into an activation function that reduces it down to a value between 1 and 0. Assuming there are no hidden layers, the sum of those values is then multiplied by the bias to get the final output."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6sWR43PTwhSk"},"source":["## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n","\n","| x1 | x2 | y |\n","|----|----|---|\n","| 0  | 0  | 1 |\n","| 1  | 0  | 1 |\n","| 0  | 1  | 1 |\n","| 1  | 1  | 0 |"]},{"cell_type":"code","metadata":{"id":"PNAuQHnKlHQx","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","data = { 'x1': [0,1,0,1],\n","         'x2': [0,0,1,1],\n","         'y':  [1,1,1,0]\n","       }\n","\n","df = pd.DataFrame.from_dict(data).astype('int')\n","\n","correct_output = np.array([[1], [1], [1], [0]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2IvHTPW-IDlK","colab":{}},"source":["# Sigmoid and its derivative\n","\n","def sigmoid(x):\n","  return 1/(1 + np.exp(-x))\n","\n","def sig_deriv(x):\n","  sx = sigmoid(x)\n","  return sx * (1 - sx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-bd8JLP3w3A","colab_type":"code","outputId":"3585c06b-29b2-4f50-d5ea-e90a942476a9","executionInfo":{"status":"ok","timestamp":1586914831154,"user_tz":240,"elapsed":743,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Initializing the weights\n","\n","weights = 2* np.random.random((3,1)) -1  \n","\n","# Get the weighted sum of the inputs\n","\n","weighted_sum = np.dot(df, weights)\n","weighted_sum"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.91922237],\n","       [1.70807832],\n","       [1.46083936],\n","       [1.33047293]])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"2vtGQNw23zID","colab_type":"code","outputId":"a021c794-cb8f-4d77-de61-ba03602f67b1","executionInfo":{"status":"ok","timestamp":1586914834544,"user_tz":240,"elapsed":1187,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Apply the sigmoid to the output to get the activated output\n","\n","act_output = sigmoid(weighted_sum)\n","\n","act_output"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.71488363],\n","       [0.84658687],\n","       [0.81166102],\n","       [0.79091885]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"7BLGVIUFAERc","colab_type":"code","outputId":"37a4e3da-2a45-4567-c109-90de8493af44","executionInfo":{"status":"ok","timestamp":1586914837428,"user_tz":240,"elapsed":570,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Find the error\n","\n","error = np.subtract(correct_output, act_output)\n","\n","error"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.28511637],\n","       [ 0.15341313],\n","       [ 0.18833898],\n","       [-0.79091885]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"zuObCe49CG8p","colab_type":"code","outputId":"bdcd8146-07a2-41df-b2e6-646946f8f92e","executionInfo":{"status":"ok","timestamp":1586914841380,"user_tz":240,"elapsed":744,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Make the backprop adjustments\n","\n","adjustments = error*sig_deriv(weighted_sum)\n","\n","weights += np.dot(df.T, adjustments)\n","\n","weights"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.6779896 ],\n","       [0.43961661],\n","       [1.02605204]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"m3J-e8umCtW3","colab_type":"code","outputId":"94b7a441-4b3d-4e33-ac74-ade542c2cc79","executionInfo":{"status":"ok","timestamp":1586914852146,"user_tz":240,"elapsed":4371,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Train the network by continuously updating the weights\n","\n","for i in range(10000):\n","  weighted_sum = np.dot(df, weights)\n","  act_output = sigmoid(weighted_sum)\n","  error = np.subtract(correct_output, act_output)\n","  adjustments = error*sig_deriv(weighted_sum)\n","  weights += np.dot(df.T, adjustments)\n","\n","print(\"Activated Output After Training:\")\n","print(act_output)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Activated Output After Training:\n","[[0.9994418 ]\n"," [0.99382226]\n"," [0.99381485]\n"," [0.00799854]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xf7sdqVs0s4x"},"source":["## Implement your own Perceptron Class and use it to classify a binary dataset: \n","- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n","\n","You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."]},{"cell_type":"code","metadata":{"id":"ITH-4sp9lHRF","colab_type":"code","outputId":"eb555c9f-5e8f-4d0d-caca-06073426bd5e","executionInfo":{"status":"ok","timestamp":1586914889426,"user_tz":240,"elapsed":746,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n","diabetes.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n","0            6      148             72  ...                     0.627   50        1\n","1            1       85             66  ...                     0.351   31        0\n","2            8      183             64  ...                     0.672   32        1\n","3            1       89             66  ...                     0.167   21        0\n","4            0      137             40  ...                     2.288   33        1\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"RmrmoDqslHRN","colab_type":"text"},"source":["Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "]},{"cell_type":"code","metadata":{"id":"EZv4gJzAMT74","colab_type":"code","outputId":"3bb40be9-ebc0-4ef6-ef45-ed602f68ed6e","executionInfo":{"status":"ok","timestamp":1586914891990,"user_tz":240,"elapsed":533,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Random test case\n","X_test = np.array([[6, 183, 66, 35, 94, 26.6, 0.672, 21]])\n","X_test"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  6.   , 183.   ,  66.   ,  35.   ,  94.   ,  26.6  ,   0.672,\n","         21.   ]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Tc2lOG2JlHRO","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler, Normalizer\n","\n","feats = list(diabetes)[:-1]\n","\n","X = diabetes[feats]\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X)\n","\n","X = scaler.transform(X)\n","X_test = scaler.transform(X_test)\n","\n","y = np.array([diabetes['Outcome']]).T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGSuwz54Jvog","colab_type":"code","outputId":"e7a711a0-cf4a-402a-ea4e-c08c3eec1a88","executionInfo":{"status":"ok","timestamp":1586914924494,"user_tz":240,"elapsed":647,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape, y.shape, X_test.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((768, 8), (768, 1), (1, 8))"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"3ade35f1-eeec-4c9b-aaf4-162192dadcde","executionInfo":{"status":"ok","timestamp":1586914929027,"user_tz":240,"elapsed":1214,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"id":"tDkH1K81QYlk","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X_test"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.35294118, 0.91959799, 0.54098361, 0.35353535, 0.11111111,\n","        0.39642325, 0.25362938, 0.        ]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-W0tiX1F1hh2","colab":{}},"source":["##### Update this Class #####\n","\n","class Perceptron:\n","    \n","    def __init__(self, niter = 10):\n","        self.niter = niter\n","    \n","    def __sigmoid(self, x):\n","        return 1/(1 + np.exp(-x))\n","    \n","    def __sigmoid_derivative(self, x):\n","        sx = sigmoid(x)\n","        return sx * (1 - sx)\n","\n","    def fit(self, X, y):\n","      \"\"\"Fit training data\n","      X : Training vectors, X.shape : [#samples, #features]\n","      y : Target values, y.shape : [#samples]\n","      \"\"\"\n","\n","      # Randomly Initialize Weights\n","      self.weights = 2*np.random.random((8, 1)) -1\n","\n","      for i in range(self.niter):\n","          # Weighted sum of inputs / weights\n","          weighted_sum = np.dot(X, self.weights)\n","          # Activate!\n","          act_output = sigmoid(weighted_sum)\n","          # Cac error\n","          error = np.subtract(y, act_output)\n","          # Update the Weights\n","          adjustments = error*sig_deriv(weighted_sum)\n","          self.weights += np.dot(X.T, adjustments)\n","\n","    def predict(self, X):\n","      weighted_sum = np.dot(X, self.weights)\n","      act_output = sigmoid(weighted_sum)\n","      return act_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGhjj975KSHW","colab_type":"code","colab":{}},"source":["nn = Perceptron()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLe_teCLLVaS","colab_type":"code","colab":{}},"source":["nn.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvPhHJtBLeAD","colab_type":"code","outputId":"90e1e6ae-06ca-4ef0-f3ae-142dd34cd6e3","executionInfo":{"status":"ok","timestamp":1586914975301,"user_tz":240,"elapsed":1166,"user":{"displayName":"Stephen Plainte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAN8tOurYPDBTRvOa8krbNzAKall1nyRPl-3yN=s64","userId":"08390977201546109578"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nn.predict(X_test)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4.48220116e-26]])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6QR4oAW1xdyu"},"source":["## Stretch Goals:\n","\n","- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n","- Implement a multi-layer perceptron. (for non-linearly separable classes)\n","- Try and implement your own backpropagation algorithm.\n","- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"]}]}